# Simplified 3D object detection for self-driving vehicles using high-definition maps
<!-- Synthetic dataset generation for Deep Learning architectures used in autonomous vehicles  -->

## Table of Contents
- [Objective](#objective)
- [Dataset](#dataset)
- [Software](#software)
- [Workflow to generate the synthetic datasets](workflow-to-generate-the-synthetic-datasets)
- [Samples of the Dataset for 3D Object Identification](samples-of-the-dataset-for-3D-object-identification)
- [Visualization of the generated grountruth bounding boxes](visualization-of-the-generated-grountruth-bounding-boxes)
- [Evaluate the models trained with the synthetic datasets](evaluate-the-models-trained-with-the-synthetic-datasets)
- [TODO](#todo)
- [Resources](#resources)

## Objective

To enable autonomous driving, the vehicle must know its surroundings. For this task, the vehicle can use both prior knowledge and sensors. One type of prior knowledge available to the system is the HD maps. These maps are high-quality, up-to-date representations of a scene, where only static scene features, e.g., road lanes and buildings, appear. If used in conjuncture with data collected by the sensors, this prior knowledge can be used to increase the systemâ€™s efficiency. One of the sensors that can be used to enable this type of gain is the LiDAR. A LiDAR is a sensor responsible for recreating the 3D scene around the vehicle that produces a collection of points.

By merging the HD maps and the data collected by the LiDAR, one can remove the background from the data fed to the machine learning model used in the autonomous vehicle's system. By doing so, the computational efficiency of the system should increase.

This project aims to process and use a dataset gathered by a LiDAR, in order to study the impact of background removal when it comes to both the precision and computation effort of the model.

<!-- In autonomous driving, the vehicle needs to have a clear understanding about its surroundings in order to avoid collisions, to follow the traffic rules and to safely drive the passengers to their destinations. To achieve these goals, there's a need for a system that can detect and classify objects, with a high level of accuracy, from multiple types of sensors. 

This project aims to process and use the data gathered from a camera and LiDAR system to provide the detection and classification of 3D objects. 

For more details check the [wiki](https://github.com/Diogo525/DL-Classification-using-3D-Point-Clouds/wiki). -->

## Dataset

The dataset used for this study is generated by simulating a LiDAR and a camera in the GTA V world. The source code of the mod responsible for the dataset generation can be found in the [script.cpp](https://github.com/joaogomes18/dataset-generation-gtav/blob/ff47581579472552cb3d0803182790502735502d/LiDAR%20GTA%20V/script.cpp) file.

After capturing the required scenes, one should use the scripts availabe in the GTADataFilter folder, in order to delete the background of the point cloud or to add context information, such as the location of the objects captured.

Afterward, and to convert the synthetic GTA V data set to the KITTI data format, the project in the GTA to KITTI dataset format conversion should be used.

<!-- The dataset of colored 3D point clouds is obtained through the simulation of a camera and LiDAR sensor in the GTA V overworld. The resolution of the resulting point clouds is specified in the [LiDAR GTA V.cfg](https://github.com/Diogo525/DL-Classification-using-3D-Point-Clouds/blob/master/Data%20processing%20scripts/LiDAR%20GTA%20V.cfg) file. The resulting GTA V mod file that accomplishes the task of data gathering is **LiDAR GTA V.asi** (found in the output directory of the project: bin/Release/LiDAR GTA V.asi). -->

## Software

- GTA V
- Blender 2.8/MeshLab
- Microsoft Visual Studio 2019
- OpenPCDet

## Workflow to generate the synthetic datasets

In order to test the generation of the three synthetic datasets, the following steps must be followed:

1. Install the Grand Theft Auto V game;
2. Setup [VAutodrive](https://www.gta5-mods.com/scripts/vautodrive);
3. Compile the project in [this](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/GTADataFilter) folder;
4. Copy the generated **LiDAR GTA V.asi** file to the game root directory;
5. In the same path, create a directory called **LiDAR GTA V** and copy the two files in the [AdditionalScripts](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/AdditionalScripts) folder to it;
6. Start the game and load the **Story Mode**
7. After the game finishes loading, the steps should be repeated:
    - Press the **F4** key, **backspace** and **F4** again;
    - Navigate the menu using the **Numpad** keys in order to make the character invunerable, as well as never wanted by the police;
    - Go to the vehicles submenu, to also make the vehicles invunerable;
    - Press **backspace**;
    - Enter a car and press **J** to activate autodrive;
    - Press **F8** to start the automoatic capturing process;
    - Once you decide to stop the process, close the game;
8. If you want to remove the background run the [where_it_stands.py](https://github.com/joaogomes18/dataset-generation-gtav/blob/ff47581579472552cb3d0803182790502735502d/GTADataFilter/where_it_stands.py) script (be carefull since this script overwrites the files);
9. If you want to add context information in the [UiConfigParams.py](https://github.com/joaogomes18/dataset-generation-gtav/blob/ff47581579472552cb3d0803182790502735502d/GTA%20to%20KITTI%20dataset%20format%20conversion/UiConfigParams.py) file replace the **includeIntensity** parameter to False, otherwise to True;
10. Run the [Main.py](https://github.com/joaogomes18/dataset-generation-gtav/blob/ff47581579472552cb3d0803182790502735502d/GTA%20to%20KITTI%20dataset%20format%20conversion/Main.py) file in order to convert the dataset to the KITTI data format;
11. Run the [correct_lbl.py](https://github.com/joaogomes18/dataset-generation-gtav/blob/ff47581579472552cb3d0803182790502735502d/GTADataFilter/correct_lbel.py) file to correct the order of the label files;
12. Lastly, run the script in file [correct_height.py](https://github.com/joaogomes18/dataset-generation-gtav/blob/7b74cc67d7521bb00466637a95d7da771b9a58b5/AdditionalScripts/Correct%203D%20bb%20height/correct_height.py) to set the 3D bounding box to the height of its 2D counterpart.

## Samples of the dataset for 3D object identification

Samples for the three datasets used to test this project can be seen in folder [Samples Dataset](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/Samples%20Dataset).

These 3 datasets correspond to the:

1. Background and foreground points;
2. Foreground only points;
3. Foreground only points with context.

## Visualization of the generated grountruth bounding boxes

In order to visualize the generated grountruth bounding boxes the scripts in folder [Generate Bounding Boxes](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/AdditionalScripts/Generate%20Bounding%20Boxes) can be used. An example of these bounding boxes can be seen in folder [ground_truth_generator](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/Samples%20Bounding%20Boxes/ground_truth_generator/all).

## Evaluate the models trained with the synthetic datasets

To train the models used with the synthetic datasets, and evaluate their performances precision-wise, the use of [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) is recomended.

To evaluate the model in computation effort, the [Perf](https://man7.org/linux/man-pages/man1/perf.1.html) command was the one selected for this project.

## Visualization of the predicted bounding boxes

In order to visualize the predicted bounding boxes the scripts in folder [Generate Bounding Boxes](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/AdditionalScripts/Generate%20Bounding%20Boxes) can be used, in addition with the files in folders **output/kitti_models/$name_of_model$/default/eval/epoch_$number_of_epochs$/val/default/final_result/data** generated by the OpenPCDet, when testing the model with the argument -save_to_dir. An example of these bounding boxes for the three generated datasets can be seen in folders [dataset1_generated](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/Samples%20Bounding%20Boxes/dataset1_generated/all), [dataset2_generated](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/Samples%20Bounding%20Boxes/dataset2_generated/all) and [dataset3_generated](https://github.com/joaogomes18/dataset-generation-gtav/tree/master/Samples%20Bounding%20Boxes/dataset3_generated/all).

## TODO

- Change the method to capture the 3D scene, from the native function used in this project to the method used in [PreSIL](https://ieeexplore.ieee.org/document/8813809);
- Hyperparameter tuning of the model used.

## Resources

- Vautodrive
     - https://www.gta5-mods.com/scripts/vautodrive
- OpenPCDet
     - https://github.com/open-mmlab/OpenPCDet
